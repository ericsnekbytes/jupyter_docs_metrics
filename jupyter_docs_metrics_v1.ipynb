{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a886faa3-06d6-4baa-9ebb-25b882c6309b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import csv\n",
    "import io\n",
    "import re\n",
    "\n",
    "from types import SimpleNamespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22cea11b-f03a-40c8-b19b-c26b0c7ff8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def csv_to_rows_of_strings(csv_string=None, filehandle=None, path=None):\n",
    "    \"\"\"Read a path/csv_string/file obj and spit out rows of string lists.\n",
    "\n",
    "    Specify ONE of csv_string, filehandle, or path (first source found in\n",
    "    that order wins if multiple are provided). Make sure to follow the csv\n",
    "    module instructions (open with newline='') and ensure the encoding is correct\n",
    "    if you provide a filehandle. Paths to files assume a utf-8 encoded file.\n",
    "    CSVs are opened with default csv module settings.\n",
    "\n",
    "    :param csv_string: str, A string containing the contents of a CSV file.\n",
    "    :param filehandle: An open file object (in string mode) to a CSV file.\n",
    "    :param path: str, Path on disk to a CSV file.\n",
    "    \"\"\"\n",
    "\n",
    "    # Dump whatever data source into a BytesIO object,\n",
    "    # then read it with the CSV reader\n",
    "    data = io.StringIO()\n",
    "    if csv_string is not None:\n",
    "        data.write(csv_string)\n",
    "    elif filehandle is not None:\n",
    "        data.write(filehandle.read())\n",
    "    elif path is not None:\n",
    "        with open(path, encoding='utf8', newline='') as csvfile:\n",
    "            data.write(csvfile.read())\n",
    "    else:\n",
    "        raise Exception(\"Must provide a source for data!\")\n",
    "    # Put seek position at 0 (like an unread file)\n",
    "    data.seek(0)\n",
    "\n",
    "    rows = []\n",
    "    reader = csv.reader(data)\n",
    "    for row in reader:\n",
    "        rows.append(row)\n",
    "    return rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c99c295-eec4-40c6-9288-400b04838799",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RowColumnView:\n",
    "    \"\"\"Lightweight row index or column-name indexable lists of cell values.\n",
    "\n",
    "    Headers are separated/removed from data rows.\n",
    "\n",
    "    Supports:\n",
    "        - for row in mydata:\n",
    "              # Do something with the row\n",
    "        - mydata.headers()\n",
    "        - len(mydata)  # Only counts data rows (not headers)\n",
    "        - Index on rows or columns:\n",
    "              mydata[51]  # Row at index 51\n",
    "              mydata['Date']  # Date column\n",
    "        - Get cells from rows or columns by column name\n",
    "              mydata[51][mydata.col_index('Date')]\n",
    "              mydata['Date'][51]\n",
    "        - \"ColumnName\" in mydata  # Check if sheet has header/column name\n",
    "        - Lazy load rows/columns with rowsi(), columni(), columnsi()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, rows_of_strings):\n",
    "        if len(rows_of_strings) < 2:\n",
    "            raise Exception('Empty CSV!')\n",
    "        self._rows = rows_of_strings[1:]\n",
    "        self._headers = rows_of_strings[0]\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        # Column names return a column\n",
    "        if isinstance(item, str):\n",
    "            if item not in self._headers:\n",
    "                raise ValueError(\"Column name must be in known headers()!\")\n",
    "            index = self._headers.index(item)\n",
    "            return [row[index] for row in self._rows]\n",
    "        elif isinstance(item, int):\n",
    "            return self._rows[int]\n",
    "        else:\n",
    "            raise ValueError(\"Must provide a string column name or row index!\")\n",
    "\n",
    "    def __contains__(self, item):\n",
    "        if item in self._rows[0]:\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._rows)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return (list(row) for row in self._rows)\n",
    "\n",
    "    def headers(self):\n",
    "        return list(self._headers)\n",
    "\n",
    "    def rowsi(self):\n",
    "        # Iterator (lazy load) over rows\n",
    "        return (list(row) for row in self._rows)\n",
    "\n",
    "    def rows(self):\n",
    "        return [list(row) for row in self.rowsi()]\n",
    "\n",
    "    def col_index(self, column_name):\n",
    "        return self._headers.index(column_name)\n",
    "\n",
    "    def columni(self, item):\n",
    "        # Iterator (lazy-load) over a column\n",
    "        if isinstance(item, str):\n",
    "            if item not in self._headers:\n",
    "                raise ValueError(\"Column name must be in known headers()!\")\n",
    "            index = self._headers.index(item)\n",
    "            return (row[index] for row in self._rows)\n",
    "        elif isinstance(item, int):\n",
    "            return (row[item] for row in self._rows)\n",
    "        else:\n",
    "            raise TypeError(\"Must provide a string column name or row index!\")\n",
    "\n",
    "    def columnsi(self):\n",
    "        # List of iterators (lazy load) for all columns\n",
    "        return [self.columni(index) for index in range(len(self._headers))]\n",
    "\n",
    "    def columns(self):\n",
    "        return [self[colname] for colname in self._headers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8b62fa-0076-4e93-b040-cd2c2ff503a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Metrics(RowColumnView):\n",
    "\n",
    "    TYPES = SimpleNamespace(\n",
    "        TRAFFIC='TRAFFIC',\n",
    "        SEARCH='SEARCH',\n",
    "    )\n",
    "    TRAFFIC_HEADERS = SimpleNamespace(  # Expected column names (in order)\n",
    "        DATE='Date',\n",
    "        VERSION='Version',\n",
    "        PATH='Path',\n",
    "        VIEWS='Views'\n",
    "    )\n",
    "    THDRS = TRAFFIC_HEADERS\n",
    "    TRAFFIC_HDR_LIST = [TRAFFIC_HEADERS.DATE, TRAFFIC_HEADERS.VERSION,\n",
    "                        TRAFFIC_HEADERS.PATH, TRAFFIC_HEADERS.VIEWS]\n",
    "    SEARCH_HEADERS = SimpleNamespace(  # Expected column names (in order)\n",
    "        CREATED_DATE='Created Date',\n",
    "        QUERY='Query',\n",
    "        TOTAL_RESULTS='Total Results',\n",
    "    )\n",
    "    SHDRS = SEARCH_HEADERS\n",
    "    SEARCH_HDR_LIST = [SEARCH_HEADERS.CREATED_DATE, SEARCH_HEADERS.QUERY,\n",
    "                       SEARCH_HEADERS.TOTAL_RESULTS]\n",
    "    INPUTS = SimpleNamespace(\n",
    "        CSV_STRING='CSV_STRING',\n",
    "        FILEHANDLE='FILEHANDLE',\n",
    "        PATH='PATH',\n",
    "    )\n",
    "\n",
    "    def __init__(self, rows_of_strings):\n",
    "        # Validate/normalize columns before instantiating\n",
    "        sheet = RowColumnView(rows_of_strings)\n",
    "        normalized_data = self._normalize_sheet(sheet)\n",
    "\n",
    "        super().__init__(normalized_data)\n",
    "\n",
    "    @staticmethod\n",
    "    def _normalize_sheet(sheet):\n",
    "        \"\"\"Take a RowColumnView and return plain rows of string lists, normalized\"\"\"\n",
    "        # Keep only expected columns in expected order\n",
    "        if (not set(sheet.headers()) >= set(Metrics.TRAFFIC_HDR_LIST)\n",
    "                and not (set(sheet.headers()) >= set(Metrics.SEARCH_HDR_LIST))):\n",
    "            raise ValueError('Must provide valid traffic or search CSV data')\n",
    "\n",
    "        # Figure out which columns we need to pull data from\n",
    "        target_headers = (\n",
    "            Metrics.TRAFFIC_HDR_LIST\n",
    "            if set(sheet.headers()) >= set(Metrics.TRAFFIC_HDR_LIST)\n",
    "            else Metrics.SEARCH_HDR_LIST\n",
    "        )\n",
    "\n",
    "        # Build rows with proper colnames and ordering\n",
    "        normalized_rows = [list(target_headers)]\n",
    "        for dirty_row in sheet:\n",
    "            norm_row = [\n",
    "                dirty_row[sheet.col_index(colname)]\n",
    "                for colname in target_headers\n",
    "            ]\n",
    "            normalized_rows.append(norm_row)\n",
    "\n",
    "        # Return string rows, for use with the base class constructor\n",
    "        return normalized_rows\n",
    "\n",
    "    def _clean_dups_and_merge(rows_of_strings):\n",
    "        # Remove exact duplicate rows, and conflicting rows for partial days where\n",
    "        # the view count is different (when date + version + path is the same but\n",
    "        # view count is different for traffic CSVs, the smaller number is a partial\n",
    "        # or earlier day so the larger number wins)...for search CSVs, same date/query\n",
    "        # are duplicates (and query match count may differ but is not relevant for metrics)\n",
    "        sheet = RowColumnView(rows_of_strings)\n",
    "        source_headers = sheet.headers()\n",
    "        if (not (set(source_headers) >= set(Metrics.SEARCH_HDR_LIST))\n",
    "                and (not set(source_headers) >= set(Metrics.TRAFFIC_HDR_LIST))):\n",
    "            raise ValueError('Cannot clean unknown CSV formats')\n",
    "        cleaned = []\n",
    "\n",
    "        # Clean exact duplicates where all columns are the same\n",
    "        no_exact_row_duplicates = [source_headers]\n",
    "        full_row_map = {}\n",
    "        for row in sheet:\n",
    "            row_tup = tuple(row)\n",
    "            if row_tup in full_row_map:\n",
    "                # Skip/don't append duplicate rows\n",
    "                continue\n",
    "            full_row_map[row_tup] = True\n",
    "            no_exact_row_duplicates.append(row)\n",
    "        # Reassign the sheet with the new data\n",
    "        sheet = RowColumnView(no_exact_row_duplicates)\n",
    "\n",
    "        no_conflicts = [source_headers]\n",
    "        # Clean conflicting/partial day rows if this is a traffic CSV\n",
    "        if set(source_headers) >= set(Metrics.TRAFFIC_HDR_LIST):\n",
    "            unique_date_vers_path_map = {\n",
    "                # Looks like\n",
    "                # (date, vers, path): [row, row2]\n",
    "                # Whole rows are stored for duplicates, most views wins\n",
    "            }\n",
    "\n",
    "            idate = sheet.col_index(Metrics.THDRS.DATE)\n",
    "            ivers = sheet.col_index(Metrics.THDRS.VERSION)\n",
    "            ipath = sheet.col_index(Metrics.THDRS.PATH)\n",
    "            iviews = sheet.col_index(Metrics.THDRS.VIEWS)\n",
    "            for row in sheet.rowsi():\n",
    "                # Find conflicts\n",
    "                fingerprint = (row[idate], row[ivers], row[ipath])\n",
    "                unique_date_vers_path_map.setdefault(fingerprint, []).append(row)\n",
    "            for row in sheet.rowsi():\n",
    "                fingerprint = (row[idate], row[ivers], row[ipath])\n",
    "                matching_rows = unique_date_vers_path_map[fingerprint]\n",
    "                if len(matching_rows) == 1:\n",
    "                    no_conflicts.append(row)\n",
    "                else:\n",
    "                    most_first = list(reversed(sorted(matching_rows, key=lambda r: int(r[iviews]))))\n",
    "                    if most_first[0] == row:\n",
    "                        no_conflicts.append(row)\n",
    "        # Search CSVs don't need conflict resolution, only duplicate resolution\n",
    "        elif set(source_headers) >= set(Metrics.SEARCH_HDR_LIST):\n",
    "            no_conflicts.extend(sheet.rowsi())\n",
    "\n",
    "        return no_conflicts\n",
    "\n",
    "    @staticmethod\n",
    "    def build(csv_string=None, filehandle=None, path=None, postproc=_clean_dups_and_merge):\n",
    "        \"\"\"Takes single items or lists of CSV sources, returns a merged sheet object\"\"\"\n",
    "        if csv_string is None and filehandle is None and path is None:\n",
    "            raise ValueError(\"Must provide a data source!\")\n",
    "\n",
    "        sources = [\n",
    "            # List of dicts, like:\n",
    "            # {\n",
    "            #      'type': Metrics.INPUTS.FILEHANDLE,\n",
    "            #      'data': RowColumnView(csv_to_rows_of_strings(foo)),\n",
    "            #      'source': src_object\n",
    "            # }\n",
    "        ]\n",
    "        if csv_string is not None:\n",
    "            if not isinstance(csv_string, list):\n",
    "                csv_string = [csv_string]\n",
    "            for cstring in csv_string:\n",
    "                sources.append({\n",
    "                    'type': Metrics.INPUTS.CSV_STRING,\n",
    "                    'data': RowColumnView(csv_to_rows_of_strings(csv_string=cstring)),\n",
    "                    'source': cstring\n",
    "                })\n",
    "        if filehandle is not None:\n",
    "            if not isinstance(filehandle, list):\n",
    "                filehandle = [filehandle]\n",
    "            for fhandle in filehandle:\n",
    "                sources.append({\n",
    "                    'type': Metrics.INPUTS.FILEHANDLE,\n",
    "                    'data': RowColuimnView(csv_to_rows_of_strings(filehandle=fhandle)),\n",
    "                    'source': fhandle\n",
    "                })\n",
    "        if path is not None:\n",
    "            if not isinstance(path, list):\n",
    "                path = [path]\n",
    "            for pth in path:\n",
    "                sources.append({\n",
    "                    'type': Metrics.INPUTS.PATH,\n",
    "                    'data': RowColumnView(csv_to_rows_of_strings(path=pth)),\n",
    "                    'source': pth\n",
    "                })\n",
    "\n",
    "        metrics_type = None\n",
    "        sources_normalized = []\n",
    "        for item in sources:\n",
    "            # Figure out which metrics type we have\n",
    "            source_sheet = item['data']\n",
    "            if metrics_type is None:\n",
    "                if set(source_sheet.headers()) >= set(Metrics.TRAFFIC_HDR_LIST):\n",
    "                    metrics_type = Metrics.TYPES.TRAFFIC\n",
    "                elif set(source_sheet.headers()) >= set(Metrics.SEARCH_HDR_LIST):\n",
    "                    metrics_type = Metrics.TYPES.SEARCH\n",
    "                else:\n",
    "                    raise ValueError(f'Error, unknown data format for {item}')\n",
    "\n",
    "            # Metrics hold (one of) either traffic data or search data,\n",
    "            # only same-types are merged\n",
    "            if ((metrics_type == Metrics.TYPES.TRAFFIC\n",
    "                    and not set(source_sheet.headers()) >= set(Metrics.TRAFFIC_HDR_LIST))\n",
    "                or (metrics_type == Metrics.TYPES.SEARCH\n",
    "                    and not set(source_sheet.headers()) >= set(Metrics.SEARCH_HDR_LIST))):\n",
    "                raise ValueError('Cannot merge disparate data types')\n",
    "\n",
    "            sheet = RowColumnView(Metrics._normalize_sheet(source_sheet))\n",
    "            if not sources_normalized:\n",
    "                # Take normalized headers from the item as first string row\n",
    "                sources_normalized.append(sheet.headers())\n",
    "            sources_normalized.extend(sheet.rowsi())\n",
    "\n",
    "        if postproc is not None:\n",
    "            sources_normalized = postproc(sources_normalized)\n",
    "\n",
    "        return Metrics(sources_normalized)\n",
    "\n",
    "    def is_traffic(self):\n",
    "        if set(self.headers()) >= set(Metrics.TRAFFIC_HDR_LIST):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def is_search(self):\n",
    "        if set(self.headers()) >= set(Metrics.SEARCH_HDR_LIST):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def total_views(self):\n",
    "        if not self.is_traffic():\n",
    "            raise Exception('Cannot get views on non-traffic data')\n",
    "\n",
    "        view_index = self.col_index(Metrics.THDRS.VIEWS)\n",
    "        return sum(int(row[view_index]) for row in self._rows)\n",
    "\n",
    "    def most_popular_queries(self, n=None):\n",
    "        if not self.is_search():\n",
    "            raise TypeError('Cannot get query counts for non-search data')\n",
    "        counts = collections.Counter()\n",
    "\n",
    "        headers = self.headers()\n",
    "        query_hdr_index = headers.index(Metrics.SHDRS.QUERY)\n",
    "        for row in self.rowsi():\n",
    "            # Each row is a search, so the row adds 1 to the count (query\n",
    "            # match count is also in each search row, but since the number\n",
    "            # of matches is not needed/relevant, we don't use it)\n",
    "            counts[row[query_hdr_index]] += 1\n",
    "        return counts.most_common() if n is None else counts.most_common(n)\n",
    "\n",
    "    def most_popular_pages(self, n=None):\n",
    "        if not self.is_traffic():\n",
    "            raise TypeError('Cannot get traffic counts for non-traffic data')\n",
    "        counts = collections.Counter()\n",
    "\n",
    "        headers = self.headers()\n",
    "        path_hdr_index = headers.index(Metrics.THDRS.PATH)\n",
    "        views_hdr_index = headers.index(Metrics.THDRS.VIEWS)\n",
    "        for row in self.rowsi():\n",
    "            counts[row[path_hdr_index]] += int(row[views_hdr_index])\n",
    "        return counts.most_common() if n is None else counts.most_common(n)\n",
    "\n",
    "    def most_popular_versions(self, n=None):\n",
    "        if not self.is_traffic():\n",
    "            raise TypeError('Cannot get version counts for non-traffic data')\n",
    "        counts = collections.Counter()\n",
    "\n",
    "        headers = self.headers()\n",
    "        path_hdr_index = headers.index(Metrics.THDRS.VERSION)\n",
    "        views_hdr_index = headers.index(Metrics.THDRS.VIEWS)\n",
    "        for row in self.rowsi():\n",
    "            counts[row[path_hdr_index]] += int(row[views_hdr_index])\n",
    "        return counts.most_common() if n is None else counts.most_common(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9dcdeab-3f3f-43b3-9297-34842653b25c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "day_a = r'readthedocs_traffic_analytics_jupyterlab_2023-12-05_2024-03-04.csv'\n",
    "day_a_plus_one = r'readthedocs_traffic_analytics_jupyterlab_2023-12-06_2024-03-05.csv'\n",
    "\n",
    "metrics_a = Metrics(csv_to_rows_of_strings(path=day_a))\n",
    "metrics_b = Metrics(csv_to_rows_of_strings(path=day_a_plus_one))\n",
    "metrics_c = Metrics.build(path=[day_a, day_a_plus_one])\n",
    "allmet = [metrics_a, metrics_b, metrics_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3aeee18c-a111-46c5-ac65-1e67a5fc0394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[525801, 526352, 533774]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.total_views() for m in allmet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b3a52d4-d4f5-44f6-96a7-41d35209d9c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('/index.html', 91722),\n",
       "  ('/getting_started/installation.html', 91100),\n",
       "  ('/getting_started/starting.html', 48675)],\n",
       " [('/index.html', 91667),\n",
       "  ('/getting_started/installation.html', 91260),\n",
       "  ('/getting_started/starting.html', 48740)],\n",
       " [('/index.html', 92985),\n",
       "  ('/getting_started/installation.html', 92459),\n",
       "  ('/getting_started/starting.html', 49412)]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.most_popular_pages(3) for m in allmet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b1c34139-a0b6-4cd5-b27f-855eec0c7638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('stable', 330250), ('latest', 147295), ('3.6.x', 21594)],\n",
       " [('stable', 330042), ('latest', 148136), ('3.6.x', 21601)],\n",
       " [('stable', 334638), ('latest', 150218), ('3.6.x', 21922)]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.most_popular_versions(3) for m in allmet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0def0615-af9d-42a6-a259-9e31c176544b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sday_a = r'readthedocs_search_analytics_jupyterlab_2023-12-05_2024-03-04.csv'\n",
    "sday_a_plus_one = r'readthedocs_search_analytics_jupyterlab_2023-12-06_2024-03-05.csv'\n",
    "\n",
    "smetrics_a = Metrics(csv_to_rows_of_strings(path=sday_a))\n",
    "smetrics_b = Metrics(csv_to_rows_of_strings(path=sday_a_plus_one))\n",
    "smetrics_c = Metrics.build(path=[sday_a, sday_a_plus_one])\n",
    "sallmet = [smetrics_a, smetrics_b, smetrics_c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fd9b5af1-73e4-4766-8163-1597b13a0e18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('password', 120),\n",
       "  ('download', 77),\n",
       "  ('kernel', 63),\n",
       "  ('config', 63),\n",
       "  ('token', 59),\n",
       "  ('shortcut', 51),\n",
       "  ('docker', 43),\n",
       "  ('markdown', 42)],\n",
       " [('password', 117),\n",
       "  ('download', 77),\n",
       "  ('kernel', 66),\n",
       "  ('config', 64),\n",
       "  ('token', 57),\n",
       "  ('shortcut', 52),\n",
       "  ('markdown', 43),\n",
       "  ('update', 42)],\n",
       " [('password', 121),\n",
       "  ('download', 78),\n",
       "  ('kernel', 66),\n",
       "  ('config', 66),\n",
       "  ('token', 59),\n",
       "  ('shortcut', 52),\n",
       "  ('markdown', 43),\n",
       "  ('docker', 43)]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m.most_popular_queries(8) for m in sallmet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9ed276-9d16-4f4c-ab7e-971c8d297cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
